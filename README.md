# Babel NLP Pipeline

A batch-processing data architecture for NLP analysis of multilingual news articles, built as part of the DLMDSEDE02 Data Engineering portfolio project.

## ğŸ—ï¸ Architecture Overview

This project implements a **Medallion Architecture** (Raw â†’ Cleaned â†’ Curated) with the following components:

| Layer | Technology | Purpose |
|-------|------------|---------|
| **Orchestration** | Apache Airflow | Monthly batch scheduling, retries, backfills |
| **Compute** | Apache Spark | ETL, NLP enrichment, quarterly aggregations |
| **Storage** | MinIO (S3-compatible) | Data lake with raw, cleaned, curated zones |
| **Data Quality** | Great Expectations | Schema validation and data quality checks |
| **Analytical DB** | ClickHouse | Low-latency queries on curated aggregates |
| **Data Delivery** | FastAPI | REST API for downstream ML workflows |

### Data Flow

```
Raw JSON â†’ Spark Ingest â†’ Raw Partitioned (Parquet)
                â†“
         Spark Clean/Enrich (NLP, Sentiment)
                â†“
         Cleaned Zone (Parquet)
                â†“
         Spark Quarterly Aggregate
                â†“
         Curated Zone (Parquet) â†’ ClickHouse â†’ FastAPI â†’ ML Consumers
```

## ğŸ“ Project Structure

```
babel-nlp-pipeline/
â”œâ”€â”€ airflow/
â”‚   â””â”€â”€ dags/
â”‚       â””â”€â”€ monthly_batch_dag.py      # Airflow DAG definition
â”œâ”€â”€ spark/
â”‚   â””â”€â”€ jobs/
â”‚       â”œâ”€â”€ ingest_all_languages.py   # Raw JSON â†’ Partitioned Parquet
â”‚       â”œâ”€â”€ clean_enrich.py           # NLP processing & sentiment analysis
â”‚       â””â”€â”€ quarterly_aggregate.py    # Quarterly aggregations
â”œâ”€â”€ fastapi/
â”‚   â”œâ”€â”€ main.py                       # REST API endpoints
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ clickhouse_init/
â”‚   â””â”€â”€ init.sql                      # Auto-creates quarterly_stats table
â”œâ”€â”€ great_expectations/
â”‚   â””â”€â”€ expectations/
â”‚       â””â”€â”€ news_suite.json           # Data quality expectations
â”œâ”€â”€ data/                             # Data lake (excluded from git)
â”‚   â”œâ”€â”€ raw/                          # Original JSON files
â”‚   â”œâ”€â”€ raw_partitioned/              # Partitioned by year/month
â”‚   â”œâ”€â”€ cleaned/                      # NLP-enriched data
â”‚   â””â”€â”€ curated/                      # Quarterly aggregates
â”œâ”€â”€ docker-compose.yml                # Infrastructure as Code
â”œâ”€â”€ generate_synthetic_data.py        # Creates test data
â””â”€â”€ .gitignore
```

## ğŸš€ Quick Start

### Prerequisites

- Docker & Docker Compose
- 8GB+ RAM recommended

### 1. Clone and Start

```bash
git clone https://github.com/Kudzo90/Batch-NLP-Data-Architecture.git
cd babel-nlp-pipeline

# Create data directories
mkdir -p data/raw data/raw_partitioned data/cleaned data/curated

# Start all services
docker-compose up -d
```

### 2. Verify Services

| Service | URL | Purpose |
|---------|-----|---------|
| Airflow | http://localhost:8080 | Orchestration UI |
| FastAPI | http://localhost:8000 | Data API |
| MinIO | http://localhost:9001 | Object storage UI |
| ClickHouse | localhost:8123 | Analytical database |

### 3. Generate Synthetic Data (Optional)

If you don't have the Babel Briefings dataset:

```bash
python generate_synthetic_data.py
```

### 4. Run the Pipeline

**Option A: Via Airflow UI**
1. Open http://localhost:8080
2. Enable the `monthly_nlp_batch` DAG
3. Trigger manually or wait for scheduled run

**Option B: Manual Spark Jobs**
```bash
# Ingest
docker exec -it spark-master /opt/spark/bin/spark-submit \
  /opt/spark-jobs/ingest_all_languages.py \
  --source /data/raw --target /data/raw_partitioned \
  --start 2020-07-01 --end 2020-08-31

# Clean & Enrich
docker exec -it spark-master /opt/spark/bin/spark-submit \
  /opt/spark-jobs/clean_enrich.py --month 2020-07

# Aggregate
docker exec -it spark-master /opt/spark/bin/spark-submit \
  /opt/spark-jobs/quarterly_aggregate.py --quarter 2020-07
```

### 5. Query the API

```bash
# List available datasets
curl http://localhost:8000/datasets

# Get Q3 2020 data
curl http://localhost:8000/quarterly/2020/3
```

## ğŸ“Š Dataset

This project uses the **Babel Briefings** dataset:
- 4.7 million multilingual news articles
- Coverage: 2017-2020 (varies by language file)
- Source: [Kaggle](https://www.kaggle.com/datasets/gpreda/babel-briefings)

The architecture is designed to scale to the full dataset, though demonstrations may use a subset due to resource constraints.

## ğŸ”§ Configuration

### Environment Variables

| Variable | Default | Description |
|----------|---------|-------------|
| `MINIO_ROOT_USER` | minioadmin | MinIO access key |
| `MINIO_ROOT_PASSWORD` | minioadmin | MinIO secret key |
| `AIRFLOW_UID` | 50000 | Airflow user ID |

### Airflow DAG Schedule

The `monthly_nlp_batch` DAG runs on the 1st of each month (`0 0 1 * *`), processing the previous month's data.

## ğŸ§ª Data Quality

Great Expectations validates:
- Schema conformance (required columns present)
- Data types (timestamps, strings, numerics)
- Value ranges (sentiment scores between -1 and 1)
- Null checks on critical fields

## ğŸ“ˆ API Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/` | GET | Health check |
| `/health` | GET | Service status |
| `/datasets` | GET | List available curated datasets |
| `/quarterly/{year}/{quarter}` | GET | Fetch quarterly aggregated data |

### Example Response

```json
{
  "year": 2020,
  "quarter": 3,
  "count": 150,
  "data": [
    {
      "category": "politics",
      "article_count": 45,
      "avg_sentiment": 0.12
    }
  ]
}
```

## ğŸ›ï¸ Design Decisions

### Why Medallion Architecture?
- Clear separation of concerns (raw â†’ cleaned â†’ curated)
- Enables reprocessing without re-ingesting
- Supports multiple downstream consumers

### Why Spark?
- Handles large-scale batch processing
- Native Parquet support
- Rich NLP libraries available

### Why ClickHouse?
- Columnar storage optimized for analytics
- Fast aggregation queries
- Scales horizontally

### Why Airflow?
- Industry-standard orchestration
- Built-in retry logic and backfills
- Visual DAG monitoring

## ğŸ”® Future Enhancements

- [ ] Implement streaming ingestion with Kafka
- [ ] Add more NLP features (NER, topic modelling)
- [ ] Deploy to Kubernetes for horizontal scaling
- [ ] Add Grafana dashboards for monitoring
- [ ] Implement CI/CD pipeline

## ğŸ“¸ Demo Outputs

The `outputs/` folder contains:
- Airflow DAG execution screenshots (Grid/Graph views)
- Airflow main page
- FastAPI endpoint responses
- Data lake folder structure verification

## ğŸ“ License

This project is submitted as part of the IU International University MSc Data Science program (DLMDSEDE02).

## ğŸ‘¤ Author

Wonder K. EKPE  
MSc Data Science  
IU International University
